{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "counter = 0\n",
    "vm_ip = \"127.0.0.1:8000\"\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "payload = {\"host\":\"localhost\", \"user\":\"root\",\n",
    "            \"password\":\"password\",\"database\":\"Urbanwood\"}\n",
    "\n",
    "srvc_url = \"http://\" + vm_ip + \"/ingestion/echo/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interacts with data ingestion service\n",
    "import json\n",
    "import requests\n",
    "\n",
    "def doc_insert(document_name, document_text, category):\n",
    "    global counter\n",
    "    counter += 1\n",
    "    \n",
    "    # insert data in payload\n",
    "    payload[\"document_name\"] = document_name\n",
    "    payload[\"document_text\"] = document_text\n",
    "    payload[\"category\"] = category\n",
    "        \n",
    "    response = requests.request(\"POST\", srvc_url, headers=headers, data=json.dumps(payload))\n",
    "    print('Document / Link - {0}: {1}'.format(counter, response.text.encode('utf8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To preprocess text of documents and blogs\n",
    "import re\n",
    "def preprocess_doc(document):\n",
    "    # getting asci characters only\n",
    "    document = ''.join([i for i in document if ord(i) < 128 and i not in ['\\n']])\n",
    "    \n",
    "    # removing extra spaces\n",
    "    document = re.sub(' +', ' ', document)\n",
    "    document = re.sub('\\n', ' ', document)\n",
    "    document = re.sub('\\t', ' ', document)\n",
    "    \n",
    "    # removing escape characters\n",
    "    document = document.replace('\\\\', '\\\\\\\\').replace('\"', '\\\\\"')\n",
    "    \n",
    "    # removing web links\n",
    "    document = re.sub(r'http\\S+', '', document)\n",
    "    \n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To parse and read pdf files\n",
    "import PyPDF2\n",
    "from tika import parser\n",
    "\n",
    "def read_pdf(file_path, pages='all', lib='pypdf'):\n",
    "    '''\n",
    "    reads all pages of file and sends back text\n",
    "    '''\n",
    "    pdf_text = ''\n",
    "    \n",
    "    if lib == 'pypdf':\n",
    "        # creating a pdf file object \n",
    "        pdfFileObj = open(file_path, 'rb') \n",
    "\n",
    "        # creating a pdf reader object \n",
    "        pdfReader = PyPDF2.PdfFileReader(pdfFileObj) \n",
    "\n",
    "        # Decrypt file\n",
    "        if pdfReader.isEncrypted:\n",
    "            pdfReader.decrypt('')\n",
    "\n",
    "        # creating a page object \n",
    "        if pages == 'all':\n",
    "            for i in range(pdfReader.numPages):\n",
    "                pageObj = pdfReader.getPage(0) \n",
    "                pdf_text += pageObj.extractText() \n",
    "\n",
    "        elif pages == 'one':\n",
    "                pageObj = pdfReader.getPage(1) \n",
    "                pdf_text += pageObj.extractText()\n",
    "\n",
    "        else:\n",
    "            raise ('Wrong no of page choice')\n",
    "\n",
    "        # closing the pdf file object \n",
    "        pdfFileObj.close()\n",
    "        \n",
    "    elif lib == 'tika':\n",
    "        file_data = parser.from_file(file_path)\n",
    "        pdf_text = file_data['content']\n",
    "    \n",
    "    else:\n",
    "        raise Exception('Wrong choice of pdf extraction library.')\n",
    "    \n",
    "    return pdf_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Recursively reading all files through directories \n",
    "import os\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "counter = 0\n",
    "for root, subdirs, files in os.walk('data/Urban Wood'):\n",
    "    i = 0\n",
    "    for file in files:\n",
    "        if '.pdf' in file:\n",
    "            file_path = root + '/' + file\n",
    "            category = '/'.join(root.split('/')[2:])\n",
    "            try:\n",
    "                document_text = read_pdf(file_path, lib='tika')\n",
    "            except Exception as e:\n",
    "                print ('Error while reading: ', file_path)\n",
    "                print ('Error: ', e)\n",
    "            \n",
    "            # preprocess document\n",
    "            if document_text:\n",
    "                document_text = preprocess_doc(document_text)\n",
    "                  \n",
    "                # Storing each document in DB one by one\n",
    "                doc_insert(file, document_text, category)\n",
    "            else:\n",
    "                print ('Document was not read: ', file_path)\n",
    "\n",
    "            # # Writing parsed documents to txt files for verification\n",
    "            # filename = 'data/read_files/' + str(file_path.replace('/','+')) + '.txt'\n",
    "            # file = open(filename, \"w+\")\n",
    "            # file.write(document_text)\n",
    "            # file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Inserting scraped data into sql\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open('data/crawl_data.json') as json_file:\n",
    "    crawl_data = json.load(json_file)\n",
    "\n",
    "counter = 0\n",
    "dfs = pd.read_excel('data/Websites.xlsx', sheet_name=None)['Sheet1']\n",
    "for category, url in zip(dfs['Category'],dfs['URL']): \n",
    "    for index, data in enumerate(crawl_data):\n",
    "        if url == data['link']:\n",
    "            # clean data\n",
    "            text = preprocess_doc(data['text'])\n",
    "            \n",
    "            # Insert into database\n",
    "            doc_insert(url+'_'+str(counter), text, category)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deprecated code, data ingestion ends here\n",
    "# # Retrieves all saved docs, to verify if docs stored or not\n",
    "# import pandas as pd\n",
    "\n",
    "# # get and display training data\n",
    "# query = \"SELECT * FROM training_data\"\n",
    "\n",
    "# ## getting records from the table\n",
    "# mycursor.execute(query)\n",
    "\n",
    "# ## fetching all records from the 'cursor' object\n",
    "# records = mycursor.fetchall()\n",
    "# field_names = [i[0] for i in mycursor.description]\n",
    "# print (field_names)\n",
    "\n",
    "# # Storing in dataframe\n",
    "# name = list()\n",
    "# text = list()\n",
    "# category = list()\n",
    "# ## Showing the data (1: name, 2: text, 3: category)\n",
    "# for index, record in enumerate(records):\n",
    "#     name.append(record[0])\n",
    "#     text.append(record[1])\n",
    "#     category.append(record[2])\n",
    "    \n",
    "# document_df = pd.DataFrame({'name':name, 'text':text, 'category':category})\n",
    "# print (document_df.shape)\n",
    "# document_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data ingestion has ended \n",
    "# Analysis (Classification using deepnet)\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn.datasets as skds\n",
    "from pathlib import Path\n",
    "\n",
    "num_labels = len(set(document_df['category']))\n",
    "vocab_size = 15000\n",
    "batch_size = 100\n",
    "encoder = LabelBinarizer()\n",
    "\n",
    "# Preparing data and target\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(document_df['text'])\n",
    " \n",
    "doc_data = tokenizer.texts_to_matrix(document_df['text'], mode='tfidf')\n",
    "doc_target = encoder.fit_transform(document_df['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Preparing and training deepnet model\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(vocab_size,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    " \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "history = model.fit(doc_data, doc_target,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=30,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deprecated code\n",
    "# # DB connection\n",
    "# import mysql.connector\n",
    "\n",
    "# mydb = mysql.connector.connect(\n",
    "#   host=\"mysql-prod3.oit.umn.edu\",\n",
    "#   user=\"cfansoaespinourbanwood\",\n",
    "#   passwd=\"2sMh$x7gyx\",\n",
    "#   database=\"cfans_oaespino_urbanwood\"\n",
    "# )\n",
    "\n",
    "# mycursor = mydb.cursor()\n",
    "\n",
    "# def doc_insert(document_name, document_text, category):\n",
    "#     # Uses globally created db connections\n",
    "#     # Inserting record\n",
    "#     query = \"INSERT INTO training_data (document_name, document_text, category) VALUES (%s, %s, %s)\"\n",
    "\n",
    "#     ## storing values in a variable\n",
    "#     values = (document_name, document_text, category)\n",
    "\n",
    "#     ## executing the query with values\n",
    "#     mycursor.execute(query, values)\n",
    "\n",
    "#     ## to make final output we have to run the 'commit()' method of the database object\n",
    "#     mydb.commit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
